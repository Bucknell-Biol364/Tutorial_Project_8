---
title: "Group Project 1"
subtitle: "Biology 368/664 Bucknell University"
output: html_notebook
author: Ian Mawn, Kaitlin McLain, and Emily Scholfield
date: 21 Feb 2022
---

# R for Dummies (BIOL208 Edition)

Welcome to this R tutorial for BIOL208! This is designed for someone who is brand new to R, so we are going to start with the very basics. To make this interesting, we'll be looking at a dataset on irises since plants are extremely important to the study of ecology and evolution. To best follow along, you should have a new markdown file open in R studio where you can copy and paste line sof code from thsi tutrotial for you to practice running on your own.

Typically, when doing data analysis in R, we want to begin by loading in the data and packages that we will use.

### Loading data and libraries
R has many built in functions that will allow us to perform all kinds of actions. However, sometimes it is useful to load some additional functions that other R users have created. We do this by loading in libraries. In the code chunk below (the grey box; this is where code is typed and run) you can see that I have loaded in some libraries.
```{r Load Libraries}
# Load other packages here.
#I just put in any libraries we might need. Don't know if we actually need them, or need more.
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
if (!require("cowplot")) install.packages("cowplot"); library(cowplot)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
if (!require("readxl")) install.packages("readxl"); library(readxl)
```

##### Libraries:
For some examples, one library is called tidyverse. It contains a ton of useful tools, but the one most relevant to this tutorial is called **ggplot**. **ggplot** allows us to create all kinds of plots to visualize our data, which you will learn more about below. You can also see the library called **cowplot**. **cowplot** is useful as it allows us to easily change the aesthetics of our graphs, and keep the designs consistent between different graphs. Finally, you can see the library **readxl**. This is useful for exactly what it sounds like... reading Microsoft Excel files! This bring us to the next step, entering data into R.

##### Data:
In the code chunk below I am going to read in some data regarding features of different species of iris (the flower). There are a few main parts of the code to think about when loading in data. First, the **read_csv()** function tells R to load in a comma separated values (csv) file with the name of the file in the parentheses. Here, you can see that our data is called "iris.csv". It is very important to type the file name exactly as it is saved on your computer, otherwise R will not be able to find it. Once the data is loaded in, we need to call it something. That is what the "IrisData <-" part of the code means. In English, it translates to: IrisData is the data found in the csv file "iris.csv". When you read in a data file, you will see it appear in your "Environment" on the right of R studio. If you click on "IrisData" in your Environment, a new tab will appear with all of the data. 
```{r Reading in Data}
IrisData <- read_csv ("iris.csv")
```

What happens if your data is in an excel file instead of a csv file? Fear not! Data can also be loaded in from an excel file using the **read_excel()** function instead of **read_csv()**. In a normal data analysis, you would only read in the data once using *either* the **read_csv()** *or* **read_excel()** functions. Your data will only read in properly if the file type matches the function you're using, so pay attention to how your data is saved before you read it in!

### Using R (the basics)
Now that we have the pieces we need, we can start learning about R. You have already seen a few aspects of R in the last section, but here we will go into some more detail about the basic functions of R to get you started with analyzing data.

This document is an R markdown file. The highlights of a markdown file is that you can have mostly regular text (not code) but still include code and their output  in specific sections. This is especially useful as it allows for lots of explanation of what the code does, like we need to do here for a tutorial. Markdowns are also convenient because they can be converted to many different formats such as a PDF (like you're reading), an html, and others.

As mentioned before, the grey sections are called code chunks. This is where the actual code is written and ran. You can insert a code chunk to your markdown file by clicking the green box with "^+^C" at the top of R studio. When you do this, a drop-down menu appears where you can choose a coding language (since this is about R, we will only use that). Refer to the image below if you can't find the code chunk button.

![Code Chunk Image](../Tutorial_Project_8/Images/CodeChunkImage.png)

In the top line of your new code chunk, you'll see **```{r}**. This can be used to name your code chunks if you wish. This can help keep your thoughts organized and is actually an important part of knitting your file...Say what??? We're going to be knitting? Don't worry, we'll explain *all* the details about knitting in the later sections of this tutorial. For now just know that naming your code chunks can help organize your thoughts and can save you problems with knitting later down the road if you make sure to name your each of your code chunks something unique.
```{r Code Chunk Example}
#Example code chunk
4+1
#This line will not run
```

Code within chunks can be run in a few ways:
1. In the top right of a chunk is a green play button that will run every line of code in the chunk.
2. Placing your cursor on a line you want to run, or highlight it, and hit command + enter (or control + enter).
3. Click the run button in the top of the screen, which will bring up a few options for running code. You can run all of the code chunks in your document at once (in order from top to bottom) with this option. This is useful for ensuring your code is functional when finalizing a document.

![Code Chunk Image](../Tutorial_Project_8/Images/RunningCode.png)

Looking at the example code chunk above, there a few points to make about coding in R. First, you can see a green line that starts with a "#". The hashtag/number/pound symbol tells R that this line is a comment, and that it should not be run as code. If you run the code chunk you will see that that line is skipped, and the only output comes from the addition found in the next line (the output should be 5 since 4+1=5). Comments are very useful for explaining your code as you write it. This is helpful for yourself in the future, so you can remind yourself how different code works, and for other people reading through or referencing your code.

Another useful bit of information is how to assign values to objects. Essentially, this is a way of naming things. You have actually already seen this in action, where we assigned our iris data to the object IrisData using the assign operator **<-**. Lets go to the next code chunk to see an example and notice how all of these values/variables will get stored in the Environment.
```{r Assign}
#Essentially, the <- means "store this value under this name," so you can think of the symbol as the word "is" or "equals." 
this <- "that"

#In the line above we are storing the word that under the name this. So the code would read in English "this is that". And if we print out the object (a.k.a. run the code) "this," it will output the word "that." Try this out yourself! In your markdown file, try running the next line, but be sure to run the line above first.
this

#Other examples
n <- 5
z <- 10 
n+z
z/n
(z+n)/3 -> five
five * 10
```

This is especially useful for storing data under a specific name, like we did at the beginning with the iris data. If you run the line of code below, it will print out all of the iris data.
```{r Printing Data}
IrisData
```

We can also access specific variables within the data using the **$** symbol. So if we only wanted to see the data about petal length, we can run the following code:
```{r Print Out Specific Variable}
IrisData$petal.length
```

Those are the basic functions you should know about R moving forward. Now we can explore more tools you can use to explore and analyze a set of data.

### Exploring Your Data
Once we have our data, it is important to explore it before beginning analysis. First, you want to come up with the question that you will attempt to answer *before analyzing your data*. This is important since we want to remain objective throughout the data analysis process.

For this tutorial, we're interested in the following questions:
1. Are sepal and petal dimensions correlated within a single species?
2. Do sepal and petal dimensions differ between Iris species?

One way to begin to understand your dataset is to run the **str()** (structure) function. This outputs lots of information about the dataset. It will show you each column of the data, the class of the column (what type of data it is: a number (num), a word (or character: chr), etc), the number of observations in the column, and some of the data within each column. This is useful for ensuring that each column contains the data you expect. For example, one column in the dataset is sepal length, so I would expect to find in the output of str() that there is a column called sepal length (it is actually called sepal.length, but that is good), it should be a number, there should be 150 observations, and the data should actually be numbers.

In the chunk below I will run str() on the iris dataset to confirm my expectations and make sure that the data looks right.
```{r Using str()}
str(IrisData)
```

One thing that stands out to me after running **str()** is the "variety" column. For starters, you may want to rename the column to something that's easier for you to understand, such as "species." You can do this using the **rename()** function as explained below. Once you've run that code, you can click on the dataset in your Environment and should see the column has been renamed.
```{r Renaming variable}
#The rename() function allows this format: DataName <- rename(DataName, NewVariableName = OldVariableName)
IrisData <- rename(IrisData, species = variety)
```

If we want to compare variables between species of irises, this column will be very useful. To make those comparisons easier, I am going to tell R that this variable is categorical. We can group the data into those categories using the as.factor() function below.
```{r Changing Variable type using as.factor()}
IrisData$species <- as.factor(IrisData$species)
#This code tells R to replace the current character species column of IrisData with the new factor species column.

str(IrisData)
#Running str() again shows that species is now a factor with 3 levels which correspond to the 3 different species.
```

It can also be useful to look at the top and bottom of your dataset using the **head()** and **tail()** functions, which show the first 6 rows and last 6 rows respectively. This is useful to ensure that the data makes sense (the data is numeric if you were expecting that) and to make sure that the correct number of rows are present (for example, make sure that the last few rows are not empty). Or, if the data is time dependent, you can use these functions to make sure that the first times are in the first row and the last times are at the end.

```{r head() and tail()}
#This chunk will output two functions, and it will separate them into two windows if you run the whole chunk. Here, the left window is the head function, and the right window is the tail function. The output windows will always proceed left to right to correspond with your code from top to bottom.
head(IrisData)
tail(IrisData)
```

A very useful function for data exploration is **summary()**. This outputs the mean, median, min, max, and first and third quartiles of any numeric data, and the number of observations within our variables. This is an important step in any data exploration. This can be useful for validating your data against external sources. If external sources show that the average sepal length of irises is 100 cm long, but our sepal lengths range from 4.3 to 7.9 cm, something might be wrong. Luckily the average is closer to 5-7 cm, so our data does make sense.

```{r summary()}
summary(IrisData)
```

### Visualization with Graphs
Now that we've spent a substantial amount of time exploring and organizing our data, we're ready to start analyzing it! The first step here is to visualize our data since up until this point, all we've been looking at are numbers (which can be hard for identifying relationships and patterns between variables). When considering what graphs may be best for visualizing our data, we should consider our questions of interest and 


### Hypothesis Testing with Statistical Tests

When using statistical tests, always be sure to check the assumptions made by the particular test you plan to use. This is important because many common statistical tests, including the two-sample t-test we will use here, assume that the datasets being compared are normally distributed. While much biological data is normally distributed, this is not always true, so be sure to explore and understand the behavior of your data before performing tests.

```{r}
#shapiro.test()
```

If the data you are using are not normally distributed, test to see if a logarithmic transformation would make the distribution normal. If so, you should use the log-transformed data. If not, continue on with the original, non-transformed data but note in your report that any statistical test results should be interpreted with caution, given that the base requirements for accurate use of the test have not been met.

```{r}
#LogPetalLength <- log(petal.length)
```

However, each test which assumes normality also has a sample size requirement which can allow for them to produce accurate results using non-normal data. For a two-sample t-test, each group is required to have a sample size of at least 15 observations to overcome the need for normality. This cutoff is different for each type of test. Thus, if your samples fit this requirement, you can proceed without worrying about normality.

### Drawing Conclusions/Additional Tools

##Explanation of p-values
A p-value represents the probability that the null hypothesis is true given the outcome observed. For most purposes, a p-value of 0.05 or lower is considered statistically significant. However, staunchly relying on this value comes with some concerns, because although it is commonly used and accepted, it is essentially an arbitrary cutoff. Thus, be sure to understand what this cutoff means so you can accurately express the strength of each result. 

Remember, a p-value of 0.05 means that there is a 5% chance of the observations being as they are if the null hypothesis is true. Thus, if you test 20 variables, it is statistically likely that at least one of them will show significance simply by chance.


























This project will require you to develop a tutorial to teach Bucknell students how to use R for graphing and data analysis. 

## Target Audience

Discuss with your group the target audience for the tutorial. 
Examples could be one of the new core Biology classes, another 300-level course (not 364), or a research group. 

```{r echo=FALSE}
IrisData <- read_csv ("iris.csv")
```

## Groups

You will work with the same groups as for the Homework 2 Peer review. 

```{r}
PeerGroups1 <- read_csv ("PeerGroups1.csv")
PeerGroups1 %>% 
  select(-MAJR, -CLAS, ) %>%
  print()
```

## Grading

Each student will be expected to complete the following tasks to earn 85% of the points available for this assignment (21/25).

- Identify and obtain suitable dataset
- Use a Github repository and version control to collaborate on the project
- Spend 4-6 hours preparing, coding, and testing tutorial
  + Data exploration
  + Data visualization
  + Hypothesis testing
- Present tutorial in class
- Provide public archive suitable for sharing to students/faculty

Tutorials from previous classes can be viewed at our public github site: https://github.com/Bucknell-Biol364

Each group should use an *Acknowledgements* section to document the participation of each member and the collaboration within and between groups.

Additional credit will be awarded for providing assistance to other groups and for the development of a tutorial that goes beyond the minimal expectations listed above.

## Sample Dataset

One of the possible datasets to use for the tutorial can be found in the datasauRus package.

```{r}
datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
      )
```

Boxplots of either the x or the y value show that there is something strange going on.

```{r}
ggplot(datasaurus_dozen, aes(x = x, colour = dataset))+
    geom_boxplot()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 3)
```

```{r}
ggplot(datasaurus_dozen, aes(x = y, colour = dataset))+
    geom_boxplot()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 3)
```

But you have to visualize all of the data with a scatter plot to really see the patterns.

```{r fig.height=12, fig.width=9}
  ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 3)
```

And did you notice the code in the {r} codechunk header that controlled the size of the output in the Rmd? Pretty neat trick!

And here are two versions of the data that you could use in your data visualization tutorial. 
To use them you would probably want to change the names of the datasets and also make x and y more meaningful. 
Then save them as a csv or tsv to be imported later for your tutorial. 

```{r}
datasaurus_long <- datasaurus_dozen
datasaurus_wide <- datasaurus_dozen_wide
head(datasaurus_long)
head(datasaurus_wide)
```

# Acknowledgements

DatasauRus package and description below: Stephanie Locke https://github.com/jumpingrivers/datasauRus

The datasauRus package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each sub-dataset has five statistics that are (almost) the same in each case. (These are the mean of x, mean of y, standard deviation of x, standard deviation of y, and Pearson correlation between x and y). However, scatter plots reveal that each sub-dataset looks very different. The dataset is intended to be used to teach students that it is important to plot their own datasets, rather than relying only on statistics.

The Datasaurus was created by Alberto Cairo in this great [blog post](http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html).

He's been subsequently made even more famous in the paper [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats) by Justin Matejka and George Fitzmaurice.

In the paper, Justin and George simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions. 

This package also makes these datasets available for use as an advanced [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet), available in R as `anscombe`.
